### **Конспект по теме: Алгоритм Хаффмана**

**Источник:** Общепринятые определения и теоремы теории кодирования, соответствующие теме раздела 1.6.6 учебника С.В. Рыбина.

---

#### **1. Основные понятия префиксного кодирования**

*   **Префиксный код (код без запятой):** Код, в котором ни одно кодовое слово не является префиксом (началом) другого. Это свойство гарантирует однозначную декодируемость без использования разделителей.
*   **Взвешенная длина пути в дереве:** Для дерева кодирования, где каждому листу (символу) $a_i$ приписана вероятность (или частота) $p_i$, а длина кодового слова (пути от корня до листа) равна $l_i$, средняя длина кода вычисляется как:
    $$
    L = \sum_{i=1}^{n} p_i \cdot l_i
    $$
    где $n$ — число символов алфавита.
*   **Задача построения оптимального префиксного кода:** Для заданного набора символов $\{a_1, ..., a_n\}$ с заданными вероятностями (частотами) $\{p_1, ..., p_n\}$ построить префиксный двоичный код с минимальной средней длиной $L$.

---

#### **2. Алгоритм Хаффмана (1952)**

Это **жадный алгоритм**, который строит оптимальный префиксный код снизу вверх, объединяя наименее вероятные символы.

**2.1. Описание алгоритма**
1.  **Инициализация:** Каждый символ $a_i$ с вероятностью $p_i$ представляет собой отдельное дерево (лист). Все деревья помещаются в приоритетную очередь, где приоритет определяется вероятностью (меньшая вероятность — высший приоритет).
2.  **Повторять** $n-1$ раз:
    a.  Извлечь из очереди два дерева $T_1$ и $T_2$ с наименьшими вероятностями $p_1$ и $p_2$.
    b.  Создать новое дерево $T$, корнем которого будет новый внутренний узел с вероятностью $p = p_1 + p_2$. Сделать $T_1$ и $T_2$ левым и правым дочерними поддеревьями корня $T$.
    c.  Поместить новое дерево $T$ с вероятностью $p$ обратно в очередь.
3.  **Результат:** Оставшееся в очереди дерево — это дерево кода Хаффмана. Кодовые слова получаются при обходе от корня к листьям: при движении по левому ребру добавляется `0`, по правому — `1`.

**2.2. Псевдокод (математическое описание)**
```
Вход: Множество символов A = {a1,...,an} и их вероятности P = {p1,...,pn}
Выход: Префиксный код, заданный деревом T

1. Инициализировать очередь приоритетов Q: для каждого i создать лист(ai, pi) и добавить в Q.
2. Для i = 1 до n-1:
    а. x = Extract-Min(Q)  // извлечь дерево с мин. вероятностью
    б. y = Extract-Min(Q)  // извлечь следующее дерево с мин. вероятностью
    в. Создать новый узел z.
       p(z) = p(x) + p(y)
       left(z) = x
       right(z) = y
    г. Insert(Q, z)        // вставить новое дерево z в очередь
3. Вернуть Extract-Min(Q)  // это корень итогового дерева T.
```

---

#### **3. Доказательство оптимальности (корректности) алгоритма**

Оптимальность алгоритма Хаффмана доказывается с использованием двух ключевых лемм.

**Лемма 1.** Пусть $a_i$ и $a_j$ — два символа с наименьшими вероятностями. Тогда существует оптимальный префиксный код, в котором кодовые слова для $a_i$ и $a_j$ имеют одинаковую длину и отличаются только последним битом (т.е. являются **братьями** в дереве кода).

*   **Идея доказательства:** Возьмём произвольный оптимальный код. Среди слов максимальной длины найдутся два, которые являются братьями (иначе можно укоротить код, удалив последний бит у слова без брата, сохранив префиксность). Эти два слова можно заменить на слова для $a_i$ и $a_j$, не увеличив среднюю длину, так как $p_i$ и $p_j$ — наименьшие.

**Лемма 2 (Принцип редукции).** Рассмотрим задачу для $n$ символов. Пусть в оптимальном коде $C$ для этой задачи символы $a_i$ и $a_j$ с наименьшими вероятностями являются братьями. Если мы **заменим** их на один новый символ $z$ с вероятностью $p_z = p_i + p_j$, то получим задачу для $n-1$ символа. Оптимальный код $C'$ для новой задачи будет иметь ту же структуру, что и код $C$, если узел $z$ "развернуть" обратно в братьев $a_i$ и $a_j$.

*   **Связь длин:** Если $L$ — средняя длина кода $C$, а $L'$ — средняя длина кода $C'$, то
    $$
    L = L' + p_z = L' + p_i + p_j.
    $$
    Добавленное слагаемое равно вкладу "удлинения" кода для $a_i$ и $a_j$ на один бит по сравнению с кодом для их родителя $z$.

**Теорема (оптимальность).** Алгоритм Хаффмана строит оптимальный префиксный код.

*   **Доказательство (индукция по n):**
    *   **База:** Для $n=2$ (два символа) код `{0, 1}` очевидно оптимален. Алгоритм строит именно его.
    *   **Переход:** Предположим, алгоритм строит оптимальный код для любого числа символов, меньшего $n$. Рассмотрим задачу для $n$ символов.
        1.  По **Лемме 1** существует оптимальный код, где два наименее вероятных символа $a_i$ и $a_j$ — братья.
        2.  Согласно **Лемме 2**, если мы объединим $a_i$ и $a_j$ в один символ $z$, то задача сведётся к построению оптимального кода для $n-1$ символа.
        3.  По предположению индукции, алгоритм Хаффмана оптимально решает эту меньшую задачу, строя код $C'$.
        4.  Алгоритм Хаффмана для исходной задачи делает ровно это: на первом шаге он объединяет $a_i$ и $a_j$, а затем рекурсивно строит код для $n-1$ символа, после чего "разворачивает" $z$ обратно в братьев. Следовательно, он строит код $C$, который в силу Леммы 2 также оптимален.

---

#### **4. Пример построения кода**

Пусть алфавит состоит из 5 символов с частотами: `A: 12, B: 4, C: 9, D: 15, E: 18`.

1.  Помещаем символы в очередь по возрастанию частот: `B(4)`, `C(9)`, `A(12)`, `D(15)`, `E(18)`.
2.  **Шаг 1:** Объединяем `B(4)` и `C(9)`. Создаём узел `X1` с частотой `13`. Очередь: `A(12)`, `X1(13)`, `D(15)`, `E(18)`.
3.  **Шаг 2:** Объединяем `A(12)` и `X1(13)`. Создаём узел `X2` с частотой `25`. Очередь: `D(15)`, `E(18)`, `X2(25)`.
4.  **Шаг 3:** Объединяем `D(15)` и `E(18)`. Создаём узел `X3` с частотой `33`. Очередь: `X2(25)`, `X3(33)`.
5.  **Шаг 4:** Объединяем `X2(25)` и `X3(33)`. Создаём корневой узел `R` с частотой `58`.

**Построим дерево и кодовые слова:**
```
        R(58)
       /     \
    X2(25)  X3(33)
    /   \    /   \
 A(12) X1  D(15) E(18)
      (13)
      /   \
    B(4)  C(9)
```
*   Присваиваем биты: левому ребру `0`, правому `1`.
*   Коды:
    *   `A`: `00`
    *   `B`: `100` (путь R→0→X2→1→X1→0→B)
    *   `C`: `101`
    *   `D`: `010`
    *   `E`: `011`

**Средняя длина кода:**
$L = (12*2 + 4*3 + 9*3 + 15*3 + 18*3) / (12+4+9+15+18) = (24+12+27+45+54)/58 = 162/58 ≈ 2.793$ бита на символ.

---

#### **5. Свойства кода Хаффмана**

1.  **Оптимальность:** Для заданного набора частот код Хаффмана имеет минимальную среднюю длину среди всех префиксных кодов.
2.  **Неоднозначность:** При наличии нескольких символов с одинаковыми вероятностями порядок объединения может быть разным, что приводит к разным, но **равнооптимальным** кодам (с одинаковой $L$).
3.  **Энтропийная граница:** Средняя длина кода Хаффмана $L$ лежит в пределах $H \le L < H + 1$, где $H = -\sum p_i \log_2 p_i$ — энтропия источника (по Шеннону). Блоковые коды Хаффмана (для групп символов) позволяют приблизить $L$ к $H$.
4.  **Сложность:** Время работы классического алгоритма — $O(n \log n)$, так как на каждом из $n-1$ шагов выполняется извлечение минимума из очереди.
5.  **Чувствительность к ошибкам:** Код не является помехоустойчивым — ошибка в одном бите может привести к неверному декодированию последующих символов (из-за префиксности).

**Заключение:** Алгоритм Хаффмана является классическим и практически важным методом сжатия данных без потерь (например, в форматах ZIP, JPEG, MP3). Его теоретическая значимость подкрепляется строгим доказательством оптимальности, основанном на жадном выборе и редукции задачи.