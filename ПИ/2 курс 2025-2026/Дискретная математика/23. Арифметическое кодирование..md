### **Конспект по теме: Арифметическое кодирование**

**Важное замечание:** В предоставленном фрагменте учебника С.В. Рыбина (стр. 1-93) тема арифметического кодирования отсутствует. Ниже представлен общепринятый теоретический конспект, соответствующий стандартному содержанию курсов теории информации и сжатия данных.

---

#### **1. Основная идея арифметического кодирования**

Арифметическое кодирование — это метод **эффективного сжатия данных без потерь**, который вместо сопоставления отдельных символов фиксированным кодовым словам (как в коде Хаффмана) **кодирует всё сообщение одним вещественным числом** в интервале $[0, 1)$.

**Ключевые отличия от кода Хаффмана:**
*   Может достигать **энтропийной границы Шеннона** для одного символа (в пределе).
*   Особенно эффективно для источников с **неравномерным распределением** вероятностей и **малыми алфавитами** (например, бинарные данные).
*   Не требует передачи дерева кодирования (если известна статистика).

---

#### **2. Алгоритм арифметического кодирования**

**2.1. Предварительные условия:**
*   Задан алфавит источника $A = \{a_1, a_2, ..., a_n\}$.
*   Известны вероятности $p_i$ (или частоты) появления каждого символа.
*   Определены **кумулятивные вероятности**:
    $$
    P_i = \sum_{j=1}^{i-1} p_j, \quad P_1 = 0, \quad P_{n+1} = 1.
    $$
    Тогда каждый символ $a_i$ соответствует полуинтервалу $[P_i, P_{i+1})$ на числовой прямой от 0 до 1.

**2.2. Процесс кодирования (последовательное сужение интервала):**
1.  Инициализация: текущий интервал $[L, R) = [0, 1)$.
2.  Для каждого символа $s_k$ входного сообщения:
    *   Определяем, какому символу $a_i$ соответствует $s_k$.
    *   **Сужаем текущий интервал** до подынтервала, соответствующего этому символу:
        $$
        \begin{aligned}
        \text{Новая левая граница: } & L' = L + (R - L) \cdot P_i, \\
        \text{Новая правая граница: } & R' = L + (R - L) \cdot P_{i+1}.
        \end{aligned}
        $$
    *   Присваиваем $[L, R) = [L', R')$.
3.  После обработки всех символов выбираем любое число $x$ из финального интервала $[L, R)$ (обычно выбирают число с минимальной двоичной записью, например, середину интервала). Это число и является **арифметическим кодом** всего сообщения.

---

#### **3. Пример кодирования**

Пусть алфавит: `A, B, C` с вероятностями: `p(A)=0.5, p(B)=0.3, p(C)=0.2`.
Кумулятивные вероятности: `P(A)=0.0, P(B)=0.5, P(C)=0.8, P(конец)=1.0`.
Закодируем сообщение `"CAB"`.

*   **Начальный интервал:** $[0, 1)$
*   **Символ `C`:** соответствует интервалу $[0.8, 1.0)$
    *   $L = 0 + (1-0)*0.8 = 0.8$
    *   $R = 0 + (1-0)*1.0 = 1.0$
    *   Новый интервал: $[0.8, 1.0)$
*   **Символ `A`:** соответствует интервалу $[0.0, 0.5)$ внутри текущего:
    *   $L = 0.8 + (1.0-0.8)*0.0 = 0.8$
    *   $R = 0.8 + (1.0-0.8)*0.5 = 0.9$
    *   Новый интервал: $[0.8, 0.9)$
*   **Символ `B`:** соответствует интервалу $[0.5, 0.8)$ внутри текущего:
    *   $L = 0.8 + (0.9-0.8)*0.5 = 0.85$
    *   $R = 0.8 + (0.9-0.8)*0.8 = 0.88$
    *   Финальный интервал: $[0.85, 0.88)$

Выбираем число из этого интервала, например, середину: `x = 0.865`. В двоичном виде `0.865 ≈ 0.110111...`. Кодом может быть любое представление этого числа, например, `110111`.

---

#### **4. Алгоритм декодирования**

Декодер работает симметрично, зная те же вероятности и кумулятивные распределения.

1.  Инициализация: текущий интервал декодирования $[L_d, R_d) = [0, 1)$.
2.  Получаем закодированное число $x$.
3.  **Пока не достигнут конец сообщения** (определяется либо специальным символом конца, либо заранее известной длиной):
    *   Определяем, в какой подынтервал текущего интервала попадает число $x$:
        $$
        \text{Найти } i: \quad L_d + (R_d - L_d) \cdot P_i \le x < L_d + (R_d - L_d) \cdot P_{i+1}.
        $$
    *   Выводим символ $a_i$.
    *   **Сужаем интервал** так же, как при кодировании:
        $$
        \begin{aligned}
        L_d' &= L_d + (R_d - L_d) \cdot P_i, \\
        R_d' &= L_d + (R_d - L_d) \cdot P_{i+1}.
        \end{aligned}
        $$
    *   Присваиваем $[L_d, R_d) = [L_d', R_d')$.
4.  Восстановлено исходное сообщение.

Для примера выше: начальный интервал $[0,1)$, число $x=0.865$.
*   Определяем первый символ: $x=0.865$ попадает в интервал для `C` $[0.8, 1.0)$ → выводим `C`.
*   Сужаем интервал до $[0.8, 1.0)$.
*   Пересчитываем позицию в новом интервале: $(0.865-0.8)/(1.0-0.8) = 0.325$.
*   $0.325$ попадает в интервал для `A` $[0.0, 0.5)$ → выводим `A`.
*   Сужаем интервал до $[0.8, 0.9)$.
*   Пересчитываем: $(0.865-0.8)/(0.9-0.8) = 0.65$.
*   $0.65$ попадает в интервал для `B` $[0.5, 0.8)$ → выводим `B`.
*   Получено `"CAB"`.

---

#### **5. Практические аспекты реализации**

**5.1. Проблема бесконечной точности:**
На практике интервалы быстро становятся очень маленькими, требуя высокой арифметической точности. Решение — **инкрементальное (поточное) кодирование**:
*   Как только старшие биты левой и правой границ становятся одинаковыми, они **фиксируются** и отправляются в выходной поток.
*   Оставшийся интервал "раздувается" в два раза, чтобы продолжать вычисления с фиксированной разрядностью.

**5.2. Масштабирование интервалов:**
Для предотвращения underflow (когда интервал становится меньше машинной точности) используются техники:
*   **E1/E2/E3 масштабирование:** При попадании интервала в одну из трёх зон ([0, 0.5), [0.5, 0.75), [0.75, 1.0)) выполняются сдвиги битов.
*   **Целочисленная арифметика:** Вероятности представляются целыми числами, интервалы вычисляются в целочисленной арифметике.

**5.3. Адаптивное арифметическое кодирование:**
Статистика источника (вероятности символов) может обновляться **динамически** в процессе кодирования/декодирования:
*   **Первоначальные вероятности** могут быть равномерными или заданными.
*   После обработки каждого символа **частоты обновляются**, что позволяет адаптироваться к неизвестной или меняющейся статистике.

---

#### **6. Сравнение с кодированием Хаффмана**

| Параметр | Код Хаффмана | Арифметическое кодирование |
|----------|--------------|----------------------------|
| **Средняя длина** | $ L \le H + 1 $ (на символ) | $ L \rightarrow H $ (на сообщение) |
| **Эффективность для малых алфавитов** | Низкая (минимум 1 бит на символ) | Высокая (может быть < 1 бита на символ) |
| **Сложность реализации** | Относительно проста | Сложнее, требует обработки переполнения |
| **Патентные ограничения** | Нет | Исторически были патентные ограничения |
| **Использование** | Широкое (ZIP, JPEG, MP3) | JPEG, JPEG2000, H.264, HEVC, Zstandard |

**Заключение:** Арифметическое кодирование — мощный метод сжатия, теоретически достигающий энтропийного предела. Его основное преимущество — возможность представлять дробное количество битов на символ, что особенно важно для источников с сильно неравномерным распределением (например, тексты на естественных языках или предсказанные остатки в видеокодеках). Современные реализации используют целочисленную арифметику с масштабированием, что делает алгоритм практичным и эффективным.